# Wine Quality Classification — ML Assignment 2

**Objective**: Build and compare multiple classification models to predict wine quality (binary: Good ≥ 6, Bad < 6), present results in a Streamlit web app, and deploy on Streamlit Community Cloud.

---

**Submission Links**
- **GitHub Repo**: Add your repo URL here before submission
- **Live Streamlit App**: Add your deployed app URL here (Streamlit Community Cloud)
- **BITS Virtual Lab Screenshot**: Include a screenshot in the final PDF

---

**Dataset Description**
- **Name**: Wine Quality — Red Wine (UCI)
- **Instances**: 1,599
- **Features**: 11 physicochemical properties (numeric)
- **Target**: `quality` converted to binary `quality_class` (Good: ≥ 6, Bad: < 6)
- **Source**: https://archive.ics.uci.edu/ml/datasets/wine+quality

Features: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol.

---

**Feature Engineering (Used in Training)**
- **Outlier removal**: Z-score filter (|z| < 3) to remove extreme values and stabilize model fits.
- **Interactions**: `acidity_ratio`, `sulfur_ratio`, `acid_score`, `alcohol_density`, `pH_alcohol`, `chlorides_sulfates`, `quality_indicators`, `balanced_acidity`, `sulfur_impact`.
- **Transforms**: Log and sqrt for skewed variables (`residual sugar`, `chlorides`, `free/total sulfur dioxide`).
- **Polynomial terms**: squared and cubed for key predictors (`alcohol`, `density`, `pH`, `sulphates`, `volatile acidity`).
- **Scaling**: MinMax normalization to [0,1] followed by `StandardScaler` on train/test split.

Impact: engineered features improved ensemble and KNN performance; KNN became best by F1/MCC while ensembles remained highly competitive.

---

**Models Used**
- Logistic Regression
- Decision Tree Classifier
- K-Nearest Neighbor Classifier
- Naive Bayes (Gaussian)
- Random Forest (Ensemble)
- XGBoost (Ensemble)

---

**Model Comparison (Test Set, 20%)**

| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |
|---------------|----------|-----|-----------|--------|----|-----|
| Logistic Regression | 0.7466 | 0.8385 | 0.8088 | 0.6962 | 0.7483 | 0.5017 |
| Decision Tree | 0.7397 | 0.7988 | 0.7562 | 0.7658 | 0.7610 | 0.4754 |
| K-Nearest Neighbor | 0.8390 | 0.9014 | 0.8405 | 0.8671 | 0.8536 | 0.6754 |
| Naive Bayes | 0.7226 | 0.8171 | 0.7984 | 0.6519 | 0.7178 | 0.4594 |
| Random Forest | 0.8151 | 0.9012 | 0.8467 | 0.8038 | 0.8247 | 0.6303 |
| XGBoost | 0.8151 | 0.9038 | 0.8467 | 0.8038 | 0.8247 | 0.6303 |

Source: generated by `train_models.py` and stored in `model/model_results.csv`.

---

**Observations**
- **KNN** shows the best overall balance (Accuracy 0.8390, F1 0.8536, MCC 0.6754) after scaling and engineered features.
- **Ensembles (RF/XGB)** remain strong and stable with high AUC (~0.90) and MCC (~0.63), robust to noise/outliers.
- **Logistic Regression** benefits from scaling and interactions; precision is high but recall lags, indicating conservative positives.
- **Decision Tree** is interpretable and competitive but lower AUC suggests weaker probability calibration.
- **Naive Bayes** is fast and decent; skew-handling helps but assumptions limit peak performance.
- Overall AUCs ≥ 0.80 demonstrate good separability of classes using physicochemical signals.

| ML Model Name | Observation about model performance |
|---------------|-------------------------------------|
| Logistic Regression | High precision, lower recall; benefits from scaling and engineered interactions. Linear decision boundary captures main effects but misses complex non-linearities; strong baseline and fast. |
| Decision Tree | Interpretable rules with decent recall; susceptible to overfitting and weaker probability calibration (lower AUC). Improved by engineered features but less robust than ensembles. |
| kNN | Best overall after scaling and feature engineering; high F1 and MCC indicate strong, balanced performance. Sensitive to scaling and outliers, but instance-based nature captures local structure well here. |
| Naive Bayes | Very fast; assumptions of feature independence limit performance on correlated physicochemical features. Skew transforms help, but still behind KNN/ensembles. |
| Random Forest (Ensemble) | Robust with high AUC and MCC; captures feature interactions and reduces variance via bagging. Stable generalization and strong metrics across the board. |
| XGBoost (Ensemble) | Competitive top-tier performance with high AUC; gradient boosting learns subtle interactions and handles class boundaries well. Slightly trails KNN in accuracy on this setup but remains excellent for deployment. |

---

**Project Structure**
```
project-folder/
│-- app.py                      # Streamlit web application (entrypoint)
│-- requirements.txt            # Python dependencies
│-- README.md                   # Documentation (includes FE and metrics)
│-- train_models.py             # Training script (writes to model/ regardless of CWD)
│-- model/                      # Saved artifacts and optional training notebooks/scripts
    │-- logistic_regression.pkl
    │-- decision_tree.pkl
    │-- k_nearest_neighbor.pkl
    │-- naive_bayes.pkl
    │-- random_forest.pkl
    │-- xgboost.pkl
    │-- scaler.pkl
    │-- model_results.csv
    │-- test_data.csv
    │-- 2025AA05810_Assignment.ipynb (optional: training/analysis notebook, if used)
```

Note: `train_models.py` now resolves the correct `model/` path using absolute directories, so artifacts generate correctly whether you run it from the project root or from inside `model/`.

---

**Run Locally**
- Create/activate a virtual environment (recommended)
- Install dependencies

```bash
pip install -r requirements.txt
```

- Train models (optional — models already included)
```bash
python train_models.py
```

- Start the app
```bash
streamlit run app.py
```

Open http://localhost:8501.

---

**Streamlit App Features**
- Dataset upload (CSV) — accepts test data with or without labels
- Model selection (dropdown) — choose among 6 models
- Metrics display — Accuracy, AUC, Precision, Recall, F1, MCC
- Confusion matrix heatmap and classification report
- Prediction distribution summary

---

**Deploy on Streamlit Community Cloud**
- Push code to GitHub
- Go to https://streamlit.io/cloud → New App → Select repo → pick branch (`main`) → file `app.py` → Deploy
- Share the live URL in the submission PDF

---

**Assignment Compliance Checklist**
- Dataset ≥ 500 instances and ≥ 12 features (engineered features used in modeling)
- 6 models implemented and evaluated with required metrics
- Interactive Streamlit app with upload, selection, metrics, confusion matrix
- README structured per assignment and included in PDF
- BITS Virtual Lab screenshot to be added in PDF

---

**References**
- P. Cortez et al., “Modeling wine preferences…,” DSS 47(4):547–553, 2009
- UCI: https://archive.ics.uci.edu/ml/datasets/wine+quality
- Streamlit: https://docs.streamlit.io
- Scikit-learn: https://scikit-learn.org

---

This README is part of the official submission for ML Assignment 2.
